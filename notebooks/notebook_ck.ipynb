{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "f06a3455",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib.ticker as mtick\n",
    "import pgeocode\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from xgboost import XGBRegressor\n",
    "import joblib\n",
    "import os\n",
    "from datetime import datetime\n",
    "#from zillow.ml_logic.data import load_data, clean_data, prepare_user_input, create_zip_dict\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "ab17061b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '__file__' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[175], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m house_df \u001b[38;5;241m=\u001b[39m \u001b[43mload_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[167], line 18\u001b[0m, in \u001b[0;36mload_data\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mload_data\u001b[39m():\n\u001b[0;32m---> 18\u001b[0m     base_dir \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mdirname(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mabspath(\u001b[38;5;18;43m__file__\u001b[39;49m))  \u001b[38;5;66;03m# .../zillow/ml_logic\u001b[39;00m\n\u001b[1;32m     19\u001b[0m     raw_data_dir \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mabspath(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(base_dir, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m..\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m..\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mraw_data\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m     21\u001b[0m     area_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(raw_data_dir, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHouseTS.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name '__file__' is not defined"
     ]
    }
   ],
   "source": [
    "house_df = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "9fe31737",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_house_df = clean_data(house_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "827ca251",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Read HouseTS.csv into area_df\n",
    "# area_df = pd.read_csv('../raw_data/HouseTS.csv')\n",
    "\n",
    "# # Read realtor-data.csv into house_df\n",
    "# house_df = pd.read_csv('../raw_data/realtor-data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c4ad3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create list of unique zipcodes in area_df\n",
    "# unique_zipcodes_area_df = area_df['zipcode'].unique().tolist()\n",
    "\n",
    "# # Filter house_df by unique_zipcoes_area_df\n",
    "# house_df = house_df[house_df['zip_code'].isin(unique_zipcodes_area_df)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "0127111a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create LTM_area_df\n",
    "# LTM_area_df = area_df[area_df['date'] == '2023-12-31'].copy()\n",
    "# LTM_area_df = LTM_area_df[['zipcode', 'Per Capita Income', 'Median Rent', 'Median Home Value', 'Median Age', 'park', 'school']]\n",
    "\n",
    "# # Rename zipcode to zip_code in LTM_area_df\n",
    "# LTM_area_df = LTM_area_df.rename(columns={'zipcode': 'zip_code', 'Per Capita Income': 'p_c_income', 'Median Rent': 'median_rent', 'Median Home Value': 'median_home_value', 'Median Age': 'median_age'})\n",
    "\n",
    "# # Merge\n",
    "# merged_df = house_df.merge(LTM_area_df, on='zip_code', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "154cda7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def clean_data(df):\n",
    "#     # Drop columns 'brokered_by', 'status'\n",
    "#     df = df.drop(columns=['brokered_by', 'status'])\n",
    "\n",
    "#      # Drop duplicates\n",
    "#     df = df.drop_duplicates()\n",
    "\n",
    "#     # Drop columns 'street', 'city', 'state' and 'prev_sold_date'\n",
    "#     df = df.drop(columns=['street', 'city', 'state', 'prev_sold_date'])\n",
    "\n",
    "#     # Drop rows with NaN values from 'price'\n",
    "#     df = df.dropna(subset=['price'])\n",
    "\n",
    "#     # Create list where 'bed' & 'bath' & 'house_size' are NaN\n",
    "#     nan_values = df[\n",
    "#         (pd.isna(df['bed'])) &\n",
    "#         (pd.isna(df['bath'])) &\n",
    "#         (pd.isna(df['house_size']))\n",
    "#     ]\n",
    "\n",
    "#     # Filter out rows that are in nan_values because we assume they are land sales\n",
    "#     df = df[~df.index.isin(nan_values.index)]\n",
    "\n",
    "#     # Impute missing data\n",
    "#     df['bed'] = df['bed'].fillna(df['bed'].median())\n",
    "#     df['bath'] = df['bath'].fillna(df['bath'].median())\n",
    "#     df['house_size'] = df['house_size'].fillna(df['house_size'].median())\n",
    "#     df['acre_lot'] = df['acre_lot'].fillna(0)\n",
    "\n",
    "#     # Calculate PPSF for each row\n",
    "#     df['ppsf'] = df['price'] / df['house_size']\n",
    "\n",
    "#     # Calculate median PPSF per zip_code\n",
    "#     ppsf_median = df.groupby('zip_code')['ppsf'].median().reset_index(name='ppsf_zipcode')\n",
    "\n",
    "#     # Merge median PPSF back to df\n",
    "#     df = df.merge(ppsf_median, on='zip_code', how='left')\n",
    "\n",
    "#     # Drop temporary ppsf column\n",
    "#     df = df.drop(columns=['ppsf'])\n",
    "\n",
    "#     # Convert zipcode to int\n",
    "#     df['zip_code'] = df['zip_code'].astype(int)\n",
    "\n",
    "#     # Calculate boundaries for 'price', 'acre_lot', 'house_size', 'ppsf_zipcode'\n",
    "#     lower_price = df['price'].quantile(0.03)\n",
    "#     upper_price = df['price'].quantile(0.97)\n",
    "#     upper_house_size = df['house_size'].quantile(0.99)\n",
    "#     lower_acre_lot = df['acre_lot'].quantile(0.01)\n",
    "#     upper_acre_lot = df['acre_lot'].quantile(0.99)\n",
    "#     lower_ppsf_zipcode = df['ppsf_zipcode'].quantile(0.03)\n",
    "#     upper_ppsf_zipcode = df['ppsf_zipcode'].quantile(0.97)\n",
    "\n",
    "#     # Apply boundaries to df\n",
    "#     df = df[\n",
    "#         (df['price'] > lower_price) &\n",
    "#         (df['price'] < upper_price) &\n",
    "#         (df['bed'] < 14) &\n",
    "#         (df['bath'] < 12) &\n",
    "#         (df['house_size'] < upper_house_size) &\n",
    "#         (df['acre_lot'] > lower_acre_lot) &\n",
    "#         (df['acre_lot'] < upper_acre_lot) &\n",
    "#         (df['ppsf_zipcode'] > lower_ppsf_zipcode) &\n",
    "#         (df['ppsf_zipcode'] < upper_ppsf_zipcode)\n",
    "#     ]\n",
    "\n",
    "#     return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f0c6e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "zipcode_dict = create_zip_dict(cleaned_house_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5919bf4b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac4ddde",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = cleaned_house_df.copy()\n",
    "df = df.drop(columns=['zip_code'], errors='ignore')\n",
    "\n",
    "target = 'price'\n",
    "features = [col for col in df.columns if col != target]\n",
    "\n",
    "X = df[features]\n",
    "y = df[target]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe9749a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define column groups\n",
    "std_cols = ['latitude', 'longitude']\n",
    "robust_cols = ['bed', 'bath', 'acre_lot', 'house_size', 'ppsf_zipcode']\n",
    "\n",
    "# Define preprocessor\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    ('std', StandardScaler(), std_cols),\n",
    "    ('rob', RobustScaler(), robust_cols)\n",
    "])\n",
    "\n",
    "# Build pipeline\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', XGBRegressor(n_estimators=100, learning_rate=0.1, random_state=42, n_jobs=-1))\n",
    "])\n",
    "\n",
    "# Train model\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Save model\n",
    "model_dir = \"model/models\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "model_path = os.path.join(model_dir, f\"{timestamp}_xgboost_pipeline.joblib\")\n",
    "\n",
    "joblib.dump(pipeline, model_path)\n",
    "\n",
    "print(f\"\\nâœ… Model saved to: {model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b1d071",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib.ticker as mtick\n",
    "import pgeocode\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from xgboost import XGBRegressor\n",
    "import joblib\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "def load_data():\n",
    "    try:\n",
    "        base_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "    except NameError:\n",
    "        base_dir = os.getcwd()\n",
    "    raw_data_dir = os.path.abspath(os.path.join(base_dir, '..', '..', 'raw_data'))\n",
    "\n",
    "    area_path = os.path.join(raw_data_dir, 'HouseTS.csv')\n",
    "    house_path = os.path.join(raw_data_dir, 'realtor-data.csv')\n",
    "\n",
    "    area_df = pd.read_csv(area_path)\n",
    "    house_df = pd.read_csv(house_path)\n",
    "\n",
    "    unique_zipcodes_area_df = area_df['zipcode'].unique().tolist()\n",
    "    house_df = house_df[house_df['zip_code'].isin(unique_zipcodes_area_df)]\n",
    "\n",
    "    return house_df\n",
    "\n",
    "\n",
    "def clean_data(df):\n",
    "    # Drop columns 'brokered_by', 'status'\n",
    "    df = df.drop(columns=['brokered_by', 'status'])\n",
    "\n",
    "     # Drop duplicates\n",
    "    df = df.drop_duplicates()\n",
    "\n",
    "    # Drop columns 'street', 'city', 'state' and 'prev_sold_date'\n",
    "    df = df.drop(columns=['street', 'city', 'state', 'prev_sold_date'])\n",
    "\n",
    "    # Drop rows with NaN values from 'price'\n",
    "    df = df.dropna(subset=['price'])\n",
    "\n",
    "    # Create list where 'bed' & 'bath' & 'house_size' are NaN\n",
    "    nan_values = df[\n",
    "        (pd.isna(df['bed'])) &\n",
    "        (pd.isna(df['bath'])) &\n",
    "        (pd.isna(df['house_size']))\n",
    "    ]\n",
    "\n",
    "    # Filter out rows that are in nan_values because we assume they are land sales\n",
    "    df = df[~df.index.isin(nan_values.index)]\n",
    "\n",
    "    # Impute missing data\n",
    "    df['bed'] = df['bed'].fillna(df['bed'].median())\n",
    "    df['bath'] = df['bath'].fillna(df['bath'].median())\n",
    "    df['house_size'] = df['house_size'].fillna(df['house_size'].median())\n",
    "    df['acre_lot'] = df['acre_lot'].fillna(0)\n",
    "\n",
    "    # Calculate PPSF for each row\n",
    "    df['ppsf'] = round(df['price'] / df['house_size'], 2)\n",
    "\n",
    "    # Calculate median PPSF per zip_code\n",
    "    ppsf_median = df.groupby('zip_code')['ppsf'].median().reset_index(name='ppsf_zipcode')\n",
    "\n",
    "    # Merge median PPSF back to df\n",
    "    df = df.merge(ppsf_median, on='zip_code', how='left')\n",
    "\n",
    "    # Convert zipcode into longitude and latitude\n",
    "    df = convert_zipcode(df)\n",
    "\n",
    "    # Drop temporary ppsf column\n",
    "    df = df.drop(columns=['ppsf'])\n",
    "\n",
    "    # Calculate boundaries for 'price', 'acre_lot', 'house_size', 'ppsf_zipcode'\n",
    "    lower_price = df['price'].quantile(0.03)\n",
    "    upper_price = df['price'].quantile(0.97)\n",
    "    upper_house_size = df['house_size'].quantile(0.99)\n",
    "    lower_acre_lot = df['acre_lot'].quantile(0.01)\n",
    "    upper_acre_lot = df['acre_lot'].quantile(0.99)\n",
    "    lower_ppsf_zipcode = df['ppsf_zipcode'].quantile(0.03)\n",
    "    upper_ppsf_zipcode = df['ppsf_zipcode'].quantile(0.97)\n",
    "\n",
    "    # Apply boundaries to df\n",
    "    df = df[\n",
    "        (df['price'] > lower_price) &\n",
    "        (df['price'] < upper_price) &\n",
    "        (df['bed'] < 14) &\n",
    "        (df['bath'] < 12) &\n",
    "        (df['house_size'] < upper_house_size) &\n",
    "        (df['acre_lot'] > lower_acre_lot) &\n",
    "        (df['acre_lot'] < upper_acre_lot) &\n",
    "        (df['ppsf_zipcode'] > lower_ppsf_zipcode) &\n",
    "        (df['ppsf_zipcode'] < upper_ppsf_zipcode)\n",
    "        ]\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def create_zip_dict(df):\n",
    "    \"\"\"\n",
    "    Create a dictionary mapping zip codes to a dict of 'ppsf_zipcode', 'longitude', and 'latitude'.\n",
    "    Example:\n",
    "        {\n",
    "            12345: {\"ppsf_zipcode\": 210.5, \"longitude\": -73.99, \"latitude\": 40.75},\n",
    "            67890: {\"ppsf_zipcode\": 198.3, \"longitude\": -74.01, \"latitude\": 40.78}\n",
    "        }\n",
    "    \"\"\"\n",
    "    # Select the needed columns and drop duplicates by zip_code\n",
    "    zip_df = df.drop_duplicates(subset=\"zip_code\")[[\"zip_code\", \"ppsf_zipcode\", \"longitude\", \"latitude\"]]\n",
    "\n",
    "    # Set zip_code as index and convert to dict of dicts\n",
    "    zip_dict = zip_df.set_index(\"zip_code\").to_dict(orient=\"index\")\n",
    "\n",
    "    return zip_dict\n",
    "\n",
    "def prepare_user_input(user_input: dict, zip_dict: dict) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Prepare user input dict into a DataFrame for prediction,\n",
    "    replacing 'zip_code' with 'ppsf_zipcode', 'longitude', and 'latitude' from zip_dict.\n",
    "\n",
    "    Args:\n",
    "        user_input: Dictionary with keys like 'bed', 'bath', 'acre_lot', 'zip_code', 'house_size'.\n",
    "        zip_dict: Dictionary mapping zip_code to dict of {'ppsf_zipcode', 'longitude', 'latitude'}.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Single-row DataFrame ready for model input.\n",
    "    \"\"\"\n",
    "    zip_info = zip_dict.get(user_input['zip_code'])\n",
    "\n",
    "    data = {\n",
    "        'bed': user_input['bed'],\n",
    "        'bath': user_input['bath'],\n",
    "        'acre_lot': user_input['acre_lot'],\n",
    "        'house_size': user_input['house_size'],\n",
    "        'ppsf_zipcode': zip_info.get('ppsf_zipcode'),\n",
    "        'longitude': zip_info.get('longitude'),\n",
    "        'latitude': zip_info.get('latitude'),\n",
    "    }\n",
    "\n",
    "    return pd.DataFrame([data])\n",
    "\n",
    "\n",
    "def convert_zipcode(df):\n",
    "    # Convert zip_code column to 5-digit string\n",
    "    df['zip_code'] = df['zip_code'].astype(str).str.replace('\\.0$', '', regex=True).str.zfill(5)\n",
    "\n",
    "    # Get unique zip codes\n",
    "    unique_zips = df['zip_code'].unique()\n",
    "\n",
    "    # Initialize pgeocode for US\n",
    "    nomi = pgeocode.Nominatim('us')\n",
    "\n",
    "    # Function to get coordinates\n",
    "    def get_coordinates(zip_code):\n",
    "        try:\n",
    "            result = nomi.query_postal_code(zip_code)\n",
    "            if result.empty or pd.isna(result.latitude):\n",
    "                return pd.Series([None, None])\n",
    "            return pd.Series([result.latitude, result.longitude])\n",
    "        except:\n",
    "            return pd.Series([None, None])\n",
    "\n",
    "    # Create DataFrame for unique zip codes\n",
    "    zip_coords = pd.DataFrame(unique_zips, columns=['zip_code'])\n",
    "    zip_coords[['latitude', 'longitude']] = zip_coords.apply(lambda row: get_coordinates(row['zip_code']), axis=1)\n",
    "\n",
    "    # Map coordinates back to filtered_house_df\n",
    "    coords_dict = zip_coords.set_index('zip_code')[['latitude', 'longitude']].to_dict('index')\n",
    "    df['latitude'] = df['zip_code'].map(lambda x: coords_dict.get(x, {}).get('latitude'))\n",
    "    df['longitude'] = df['zip_code'].map(lambda x: coords_dict.get(x, {}).get('longitude'))\n",
    "\n",
    "    # Drop 'zip_code' column\n",
    "    df = df.drop(columns=['zip_code'])\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bab1c0e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zillows_real_estate",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
