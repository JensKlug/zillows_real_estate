{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d78f7e7b-04fb-40dd-9f56-73c7e67254a2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-10T14:22:33.268879Z",
     "iopub.status.busy": "2025-06-10T14:22:33.268763Z",
     "iopub.status.idle": "2025-06-10T14:22:33.455212Z",
     "shell.execute_reply": "2025-06-10T14:22:33.454420Z",
     "shell.execute_reply.started": "2025-06-10T14:22:33.268864Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib.ticker as mtick\n",
    "import pgeocode\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from xgboost import XGBRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "331c152c-c6eb-4d70-9229-7c374ded6099",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-10T14:22:33.456105Z",
     "iopub.status.busy": "2025-06-10T14:22:33.455852Z",
     "iopub.status.idle": "2025-06-10T14:22:34.583836Z",
     "shell.execute_reply": "2025-06-10T14:22:34.583170Z",
     "shell.execute_reply.started": "2025-06-10T14:22:33.456088Z"
    }
   },
   "outputs": [],
   "source": [
    "# Read HouseTS.csv into area_df\n",
    "area_df = pd.read_csv('../raw_data/HouseTS.csv')\n",
    "\n",
    "# Read realtor-data.csv into house_df\n",
    "house_df = pd.read_csv('../raw_data/realtor-data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8822cafa-2d08-4b3c-ac6a-61eb8ebf0967",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-10T14:22:34.585476Z",
     "iopub.status.busy": "2025-06-10T14:22:34.585119Z",
     "iopub.status.idle": "2025-06-10T14:22:34.597243Z",
     "shell.execute_reply": "2025-06-10T14:22:34.596693Z",
     "shell.execute_reply.started": "2025-06-10T14:22:34.585457Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 95 μs, sys: 21 μs, total: 116 μs\n",
      "Wall time: 121 μs\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>brokered_by</th>\n",
       "      <th>status</th>\n",
       "      <th>price</th>\n",
       "      <th>bed</th>\n",
       "      <th>bath</th>\n",
       "      <th>acre_lot</th>\n",
       "      <th>street</th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "      <th>zip_code</th>\n",
       "      <th>house_size</th>\n",
       "      <th>prev_sold_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>103378.0</td>\n",
       "      <td>for_sale</td>\n",
       "      <td>105000.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.12</td>\n",
       "      <td>1962661.0</td>\n",
       "      <td>Adjuntas</td>\n",
       "      <td>Puerto Rico</td>\n",
       "      <td>601.0</td>\n",
       "      <td>920.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>52707.0</td>\n",
       "      <td>for_sale</td>\n",
       "      <td>80000.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.08</td>\n",
       "      <td>1902874.0</td>\n",
       "      <td>Adjuntas</td>\n",
       "      <td>Puerto Rico</td>\n",
       "      <td>601.0</td>\n",
       "      <td>1527.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>103379.0</td>\n",
       "      <td>for_sale</td>\n",
       "      <td>67000.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.15</td>\n",
       "      <td>1404990.0</td>\n",
       "      <td>Juana Diaz</td>\n",
       "      <td>Puerto Rico</td>\n",
       "      <td>795.0</td>\n",
       "      <td>748.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>31239.0</td>\n",
       "      <td>for_sale</td>\n",
       "      <td>145000.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.10</td>\n",
       "      <td>1947675.0</td>\n",
       "      <td>Ponce</td>\n",
       "      <td>Puerto Rico</td>\n",
       "      <td>731.0</td>\n",
       "      <td>1800.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>34632.0</td>\n",
       "      <td>for_sale</td>\n",
       "      <td>65000.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.05</td>\n",
       "      <td>331151.0</td>\n",
       "      <td>Mayaguez</td>\n",
       "      <td>Puerto Rico</td>\n",
       "      <td>680.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   brokered_by    status     price  bed  bath  acre_lot     street  \\\n",
       "0     103378.0  for_sale  105000.0  3.0   2.0      0.12  1962661.0   \n",
       "1      52707.0  for_sale   80000.0  4.0   2.0      0.08  1902874.0   \n",
       "2     103379.0  for_sale   67000.0  2.0   1.0      0.15  1404990.0   \n",
       "3      31239.0  for_sale  145000.0  4.0   2.0      0.10  1947675.0   \n",
       "4      34632.0  for_sale   65000.0  6.0   2.0      0.05   331151.0   \n",
       "\n",
       "         city        state  zip_code  house_size prev_sold_date  \n",
       "0    Adjuntas  Puerto Rico     601.0       920.0            NaN  \n",
       "1    Adjuntas  Puerto Rico     601.0      1527.0            NaN  \n",
       "2  Juana Diaz  Puerto Rico     795.0       748.0            NaN  \n",
       "3       Ponce  Puerto Rico     731.0      1800.0            NaN  \n",
       "4    Mayaguez  Puerto Rico     680.0         NaN            NaN  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create list of unique zipcodes in area_df\n",
    "unique_zipcodes_area_df = area_df['zipcode'].unique().tolist()\n",
    "\n",
    "# Filter house_df by unique_zipcoes_area_df\n",
    "house_df = house_df[house_df['zip_code'].isin(unique_zipcodes_area_df)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a09e7f-1781-4e8a-9e07-757412072781",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-10T14:22:34.598001Z",
     "iopub.status.busy": "2025-06-10T14:22:34.597852Z",
     "iopub.status.idle": "2025-06-10T14:22:35.907030Z",
     "shell.execute_reply": "2025-06-10T14:22:35.906426Z",
     "shell.execute_reply.started": "2025-06-10T14:22:34.597987Z"
    }
   },
   "outputs": [],
   "source": [
    "def clean_data(df):\n",
    "    # Drop columns 'brokered_by', 'status'\n",
    "    df = df.drop(columns=['brokered_by', 'status'])\n",
    "\n",
    "     # Drop duplicates\n",
    "    df = df.drop_duplicates()\n",
    "\n",
    "    # Drop columns 'street', 'city', 'state' and 'prev_sold_date'\n",
    "    df = df.drop(columns=['street', 'city', 'state', 'prev_sold_date'])\n",
    "\n",
    "    # Drop rows with NaN values from 'price'\n",
    "    df = df.dropna(subset=['price'])\n",
    "\n",
    "    # Create list where 'bed' & 'bath' & 'house_size' are NaN\n",
    "    nan_values = df[\n",
    "        (pd.isna(df['bed'])) &\n",
    "        (pd.isna(df['bath'])) &\n",
    "        (pd.isna(df['house_size']))\n",
    "    ]\n",
    "\n",
    "    # Filter out rows that are in nan_values because we assume they are land sales\n",
    "    df = df[~df.index.isin(nan_values.index)]\n",
    "\n",
    "    # Impute missing data\n",
    "    df['bed'] = df['bed'].fillna(df['bed'].median())\n",
    "    df['bath'] = df['bath'].fillna(df['bath'].median())\n",
    "    df['house_size'] = df['house_size'].fillna(df['house_size'].median())\n",
    "    df['acre_lot'] = df['acre_lot'].fillna(0)\n",
    "\n",
    "    # Step 2: Calculate PPSF for each row\n",
    "    df['ppsf'] = df['price'] / df['house_size']\n",
    "\n",
    "    # Step 3: Calculate median PPSF per zip_code\n",
    "    ppsf_median = df.groupby('zip_code')['ppsf'].median().reset_index(name='ppsf_zipcode')\n",
    "\n",
    "    # Step 4: Merge median PPSF back to df\n",
    "    df = df.merge(ppsf_median, on='zip_code', how='left')\n",
    "\n",
    "    # Drop temporary ppsf column\n",
    "    df = df.drop(columns=['ppsf'])\n",
    "\n",
    "    # Calculate boundaries for 'price', 'acre_lot', 'house_size', 'ppsf_zipcode'\n",
    "    lower_price = df['price'].quantile(0.03)\n",
    "    upper_price = df['price'].quantile(0.97)\n",
    "    upper_house_size = df['house_size'].quantile(0.99)\n",
    "    lower_acre_lot = df['acre_lot'].quantile(0.01)\n",
    "    upper_acre_lot = df['acre_lot'].quantile(0.99)\n",
    "    lower_ppsf_zipcode = df['ppsf_zipcode'].quantile(0.03)\n",
    "    upper_ppsf_zipcode = df['ppsf_zipcode'].quantile(0.97)\n",
    "\n",
    "    # Apply boundaries to df\n",
    "    df = df[\n",
    "        (df['price'] > lower_price) &\n",
    "        (df['price'] < upper_price) &\n",
    "        (df['bed'] < 14) &\n",
    "        (df['bath'] < 12) &\n",
    "        (df['house_size'] < upper_house_size) &\n",
    "        (df['acre_lot'] > lower_acre_lot) &\n",
    "        (df['acre_lot'] < upper_acre_lot) &\n",
    "        (df['ppsf_zipcode'] > lower_ppsf_zipcode) &\n",
    "        (df['ppsf_zipcode'] < upper_ppsf_zipcode)\n",
    "        ]\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53975bc0-a71e-4ce0-9ef1-7e45f9d18769",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-10T14:22:35.907756Z",
     "iopub.status.busy": "2025-06-10T14:22:35.907624Z",
     "iopub.status.idle": "2025-06-10T14:22:35.914094Z",
     "shell.execute_reply": "2025-06-10T14:22:35.913513Z",
     "shell.execute_reply.started": "2025-06-10T14:22:35.907744Z"
    }
   },
   "outputs": [],
   "source": [
    "# Clean df\n",
    "cleaned_house_df = clean_data(house_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c43d04d4-09c0-4735-87b5-252ed4863580",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-10T14:22:35.915110Z",
     "iopub.status.busy": "2025-06-10T14:22:35.914925Z",
     "iopub.status.idle": "2025-06-10T14:22:36.201894Z",
     "shell.execute_reply": "2025-06-10T14:22:36.201062Z",
     "shell.execute_reply.started": "2025-06-10T14:22:35.915095Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 936724 entries, 5199 to 2224341\n",
      "Data columns (total 12 columns):\n",
      " #   Column          Non-Null Count   Dtype  \n",
      "---  ------          --------------   -----  \n",
      " 0   brokered_by     935597 non-null  float64\n",
      " 1   status          936724 non-null  object \n",
      " 2   price           936401 non-null  float64\n",
      " 3   bed             844470 non-null  float64\n",
      " 4   bath            833568 non-null  float64\n",
      " 5   acre_lot        755663 non-null  float64\n",
      " 6   street          931960 non-null  float64\n",
      " 7   city            936587 non-null  object \n",
      " 8   state           936724 non-null  object \n",
      " 9   zip_code        936724 non-null  float64\n",
      " 10  house_size      793101 non-null  float64\n",
      " 11  prev_sold_date  735667 non-null  object \n",
      "dtypes: float64(8), object(4)\n",
      "memory usage: 92.9+ MB\n"
     ]
    }
   ],
   "source": [
    "def convert_zipcode(df):\n",
    "    # Convert zip_code column to 5-digit string\n",
    "    df['zip_code'] = df['zip_code'].astype(str).str.replace('\\.0$', '', regex=True).str.zfill(5)\n",
    "\n",
    "    # Get unique zip codes\n",
    "    unique_zips = df['zip_code'].unique()\n",
    "\n",
    "    # Initialize pgeocode for US\n",
    "    nomi = pgeocode.Nominatim('us')\n",
    "\n",
    "    # Function to get coordinates\n",
    "    def get_coordinates(zip_code):\n",
    "        try:\n",
    "            result = nomi.query_postal_code(zip_code)\n",
    "            if result.empty or pd.isna(result.latitude):\n",
    "                return pd.Series([None, None])\n",
    "            return pd.Series([result.latitude, result.longitude])\n",
    "        except:\n",
    "            return pd.Series([None, None])\n",
    "\n",
    "    # Create DataFrame for unique zip codes\n",
    "    zip_coords = pd.DataFrame(unique_zips, columns=['zip_code'])\n",
    "    zip_coords[['latitude', 'longitude']] = zip_coords.apply(lambda row: get_coordinates(row['zip_code']), axis=1)\n",
    "\n",
    "    # Map coordinates back to filtered_house_df\n",
    "    coords_dict = zip_coords.set_index('zip_code')[['latitude', 'longitude']].to_dict('index')\n",
    "    df['latitude'] = df['zip_code'].map(lambda x: coords_dict.get(x, {}).get('latitude'))\n",
    "    df['longitude'] = df['zip_code'].map(lambda x: coords_dict.get(x, {}).get('longitude'))\n",
    "\n",
    "    # Drop 'zip_code' column\n",
    "    df = df.drop(columns=['zip_code'])\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c98d6d-a252-4144-a8b1-998e5dd98037",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-10T14:22:36.203043Z",
     "iopub.status.busy": "2025-06-10T14:22:36.202597Z",
     "iopub.status.idle": "2025-06-10T14:22:36.228077Z",
     "shell.execute_reply": "2025-06-10T14:22:36.223684Z",
     "shell.execute_reply.started": "2025-06-10T14:22:36.203019Z"
    }
   },
   "outputs": [],
   "source": [
    "# Convert zipcodes to coordinates\n",
    "cleaned_house_df = convert_zipcode(cleaned_house_df)\n",
    "#cleaned_house_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0bded8c-4aab-4f69-ba1a-d4c58d9b204c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-10T14:22:36.229853Z",
     "iopub.status.busy": "2025-06-10T14:22:36.228746Z",
     "iopub.status.idle": "2025-06-10T14:22:36.584255Z",
     "shell.execute_reply": "2025-06-10T14:22:36.583592Z",
     "shell.execute_reply.started": "2025-06-10T14:22:36.229836Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             price  bed  bath  acre_lot     street        city          state  \\\n",
      "24595     659900.0  NaN   NaN    106.86  1853232.0  Barrington  New Hampshire   \n",
      "44736     449000.0  5.0   3.0      0.06   893062.0  Bloomfield     New Jersey   \n",
      "46234     329999.0  4.0   2.0      0.03  1260708.0     Haledon     New Jersey   \n",
      "46583     524900.0  5.0   3.0      0.06   984852.0        Lodi     New Jersey   \n",
      "46767     499000.0  5.0   3.0      0.05  1372844.0    Brooklyn       New York   \n",
      "...            ...  ...   ...       ...        ...         ...            ...   \n",
      "2197155  1025000.0  3.0   2.0      0.03  1374909.0     Seattle     Washington   \n",
      "2197253   680000.0  3.0   2.0      0.08   111278.0      Renton     Washington   \n",
      "2197789   775000.0  3.0   2.0      0.05   194389.0     Seattle     Washington   \n",
      "2202651   549500.0  5.0   3.0      0.08   133822.0    Puyallup     Washington   \n",
      "2203665  1897000.0  3.0   3.0      0.48  1146365.0   Sammamish     Washington   \n",
      "\n",
      "         zip_code  house_size prev_sold_date  \n",
      "24595      3825.0         NaN            NaN  \n",
      "44736      7003.0         NaN            NaN  \n",
      "46234      7508.0         NaN            NaN  \n",
      "46583      7644.0         NaN            NaN  \n",
      "46767     11210.0      2144.0            NaN  \n",
      "...           ...         ...            ...  \n",
      "2197155   98107.0      1643.0     2022-03-24  \n",
      "2197253   98055.0      1340.0     2022-03-24  \n",
      "2197789   98133.0      2580.0     2022-04-01  \n",
      "2202651   98374.0      2375.0     2022-03-24  \n",
      "2203665   98075.0      3040.0     2022-04-26  \n",
      "\n",
      "[1392 rows x 10 columns]\n"
     ]
    }
   ],
   "source": [
    "cleaned_house_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8dc2e46-320f-480a-9cf2-88e371fa862c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-10T14:22:36.585087Z",
     "iopub.status.busy": "2025-06-10T14:22:36.584784Z",
     "iopub.status.idle": "2025-06-10T14:22:36.876081Z",
     "shell.execute_reply": "2025-06-10T14:22:36.875436Z",
     "shell.execute_reply.started": "2025-06-10T14:22:36.585072Z"
    }
   },

   "outputs": [],
   "source": [
    "# # Work on a copy to avoid SettingWithCopyWarning\n",
    "# df = cleaned_house_df.copy()\n",
    "# df = df.drop(columns=['latitude', 'longitude'])\n",
    "\n",
    "# # Define features and target\n",
    "# target = 'price'\n",
    "# features = [col for col in df.columns if col != target]  # Exclude price\n",
    "# numeric_features = [col for col in features if col != 'zip_code']  # Exclude zip_code\n",
    "\n",
    "# # Verify columns\n",
    "# print(\"\\nFeatures:\", features)\n",
    "# print(\"Numeric features for scaling:\", numeric_features)\n",
    "# if target not in df.columns:\n",
    "#     raise ValueError(f\"'{target}' column not found. Available columns: {df.columns.tolist()}\")\n",
    "\n",
    "# # Create X and y\n",
    "# X = df[features]\n",
    "# y = df[target]\n",
    "\n",
    "# # Preprocess with ColumnTransformer\n",
    "# preprocessor = ColumnTransformer(\n",
    "#     transformers=[\n",
    "#         ('num', StandardScaler(), numeric_features)\n",
    "#     ],\n",
    "#     remainder='passthrough'  # Keep zip_code unscaled\n",
    "# )\n",
    "\n",
    "# # Split data\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# # Create pipeline\n",
    "# from sklearn.pipeline import Pipeline\n",
    "# pipeline = Pipeline([\n",
    "#     ('preprocessor', preprocessor),\n",
    "#     ('regressor', XGBRegressor(n_estimators=100, learning_rate=0.1, random_state=42, n_jobs=-1))\n",
    "# ])\n",
    "\n",
    "# # Train model\n",
    "# pipeline.fit(X_train, y_train)\n",
    "\n",
    "# # Predict and evaluate\n",
    "# y_pred = pipeline.predict(X_test)\n",
    "\n",
    "# # Calculate metrics\n",
    "# rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "# mae = mean_absolute_error(y_test, y_pred)\n",
    "# r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "# # Print results\n",
    "# print(\"\\nModel Results:\")\n",
    "# print(f\"XGBoost RMSE: ${rmse:,.2f}\")\n",
    "# print(f\"Mean Absolute Error (MAE): ${mae:,.2f}\")\n",
    "# print(f\"R² Score: {r2:.4f}\")\n",
    "\n",
    "# # Feature importance\n",
    "# feature_names = numeric_features + ['zip_code']\n",
    "# print(\"\\nFeature Importance:\")\n",
    "# for name, importance in zip(feature_names, pipeline.named_steps['regressor'].feature_importances_):\n",
    "#     print(f\"{name}: {importance:.4f}\")\n",
    "\n",
    "# # Sample of actual vs. predicted prices\n",
    "# results_df = pd.DataFrame({\n",
    "#     'Actual Price': y_test,\n",
    "#     'Predicted Price': y_pred,\n",
    "#     'Difference': y_test - y_pred\n",
    "# })\n",
    "# print(\"\\nSample of Actual vs. Predicted Prices:\")\n",
    "# print(results_df.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "beba8da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# from sklearn.compose import ColumnTransformer\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "# from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "# from sklearn.pipeline import Pipeline\n",
    "# from xgboost import XGBRegressor\n",
    "\n",
    "# # 1. Work on a copy to avoid SettingWithCopyWarning\n",
    "# df = cleaned_house_df.copy()\n",
    "# df = df.drop(columns=['latitude', 'longitude'])\n",
    "\n",
    "# # 2. Define features and target\n",
    "# target = 'price'\n",
    "# features = [col for col in df.columns if col != target]  # Exclude price\n",
    "# numeric_features = [col for col in features if col != 'zip_code']  # Exclude zip_code\n",
    "\n",
    "# # Verify columns\n",
    "# print(\"\\nFeatures:\", features)\n",
    "# print(\"Numeric features for scaling:\", numeric_features)\n",
    "# if target not in df.columns:\n",
    "#     raise ValueError(f\"'{target}' column not found. Available columns: {df.columns.tolist()}\")\n",
    "\n",
    "# # 3. Create X and y\n",
    "# X = df[features]\n",
    "# y = df[target]\n",
    "\n",
    "# # 4. Preprocess with ColumnTransformer\n",
    "# preprocessor = ColumnTransformer(\n",
    "#     transformers=[\n",
    "#         ('num', StandardScaler(), numeric_features)  # Scale numeric features\n",
    "#     ],\n",
    "#     remainder='passthrough'  # Keep zip_code unscaled\n",
    "# )\n",
    "\n",
    "# # 5. Split data\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# # 6. Create pipeline with XGBoost\n",
    "# pipeline = Pipeline([\n",
    "#     ('preprocessor', preprocessor),\n",
    "#     ('regressor', XGBRegressor(random_state=42, n_jobs=-1))\n",
    "# ])\n",
    "\n",
    "# # 7. Define hyperparameter grid for GridSearchCV\n",
    "# param_grid = {\n",
    "#     'regressor__n_estimators': [100, 200],  # Number of trees\n",
    "#     'regressor__learning_rate': [0.01, 0.1],  # Step size for boosting\n",
    "#     'regressor__max_depth': [3, 5],  # Depth of trees\n",
    "#     'regressor__min_child_weight': [1, 3]  # Minimum sum of instance weight needed in a child\n",
    "# }\n",
    "\n",
    "# # 8. Perform GridSearchCV\n",
    "# grid_search = GridSearchCV(\n",
    "#     pipeline,\n",
    "#     param_grid,\n",
    "#     cv=3,  # 3-fold cross-validation\n",
    "#     scoring='neg_mean_squared_error',\n",
    "#     n_jobs=-1,  # Use all cores\n",
    "#     verbose=1\n",
    "# )\n",
    "\n",
    "# # 9. Fit the model\n",
    "# grid_search.fit(X_train, y_train)\n",
    "\n",
    "# # 10. Evaluate\n",
    "# best_model = grid_search.best_estimator_\n",
    "# y_pred = best_model.predict(X_test)\n",
    "\n",
    "# rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "# mae = mean_absolute_error(y_test, y_pred)\n",
    "# r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "# # Print results\n",
    "# print(\"\\nModel Results:\")\n",
    "# print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "# print(f\"Best cross-validated RMSE: ${np.sqrt(-grid_search.best_score_):,.2f}\")\n",
    "# print(f\"Test RMSE: ${rmse:,.2f}\")\n",
    "# print(f\"Mean Absolute Error (MAE): ${mae:,.2f}\")\n",
    "# print(f\"R² Score: {r2:.4f}\")\n",
    "\n",
    "# # 11. Feature importance\n",
    "# feature_names = numeric_features + ['zip_code']\n",
    "# regressor = best_model.named_steps['regressor']\n",
    "# print(\"\\nFeature Importance:\")\n",
    "# for name, importance in zip(feature_names, regressor.feature_importances_):\n",
    "#     print(f\"{name}: {importance:.4f}\")\n",
    "\n",
    "# # 12. Sample of actual vs. predicted prices\n",
    "# results_df = pd.DataFrame({\n",
    "#     'Actual Price': y_test,\n",
    "#     'Predicted Price': y_pred,\n",
    "#     'Difference': y_test - y_pred\n",
    "# })\n",
    "# print(\"\\nSample of Actual vs. Predicted Prices:\")\n",
    "# print(results_df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "000e0932",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test\n"
     ]
    }
   ],
   "source": [
    "print(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "93cea8bb",
   "metadata": {},

    
   "outputs": [
    {
     "data": {
      "text/plain": [
       "935332"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Work on a copy to avoid SettingWithCopyWarning\n",
    "df = cleaned_house_df.copy()\n",
    "df = df.drop(columns=['latitude', 'longitude'])\n",
    "\n",
    "# Define features and target\n",
    "target = 'price'\n",
    "features = [col for col in df.columns if col != target]  # Exclude price\n",
    "numeric_features = [col for col in features if col != 'zip_code']  # Exclude zip_code\n",
    "\n",
    "# Verify columns\n",
    "print(\"\\nFeatures:\", features)\n",
    "print(\"Numeric features for scaling:\", numeric_features)\n",
    "if target not in df.columns:\n",
    "    raise ValueError(f\"'{target}' column not found. Available columns: {df.columns.tolist()}\")\n",
    "\n",
    "# Create X and y\n",
    "X = df[features]\n",
    "y = df[target]\n",
    "\n",
    "# Preprocess with ColumnTransformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numeric_features)\n",
    "    ],\n",
    "    remainder='passthrough'  # Keep zip_code unscaled\n",
    ")\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', XGBRegressor(n_estimators=100, learning_rate=0.1, random_state=42, n_jobs=-1))\n",
    "])\n",
    "\n",
    "# Train model\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "# Calculate metrics\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "# Print results\n",
    "print(\"\\nModel Results:\")\n",
    "print(f\"XGBoost RMSE: ${rmse:,.2f}\")\n",
    "print(f\"Mean Absolute Error (MAE): ${mae:,.2f}\")\n",
    "print(f\"R² Score: {r2:.4f}\")\n",
    "\n",
    "# Feature importance\n",
    "feature_names = numeric_features + ['zip_code']\n",
    "print(\"\\nFeature Importance:\")\n",
    "for name, importance in zip(feature_names, pipeline.named_steps['regressor'].feature_importances_):\n",
    "    print(f\"{name}: {importance:.4f}\")\n",
    "\n",
    "# Sample of actual vs. predicted prices\n",
    "results_df = pd.DataFrame({\n",
    "    'Actual Price': y_test,\n",
    "    'Predicted Price': y_pred,\n",
    "    'Difference': y_test - y_pred\n",
    "})\n",
    "print(\"\\nSample of Actual vs. Predicted Prices:\")\n",
    "print(results_df.head(10))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zillows_real_estate",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
