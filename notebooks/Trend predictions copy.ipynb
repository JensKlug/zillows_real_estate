{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e062bdeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import precision_score, confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b7d529e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Returns just one month but we need 3 months.\n",
    "\n",
    "\n",
    "df = pd.read_csv('/home/jensk/code/JensKlug/zillows_real_estate/raw_data/HouseTS.csv')\n",
    "\n",
    "df = df.sort_values(['city', 'zipcode', 'date']).reset_index(drop=True)\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "df['year_month'] = df['date'].dt.to_period('M')\n",
    "\n",
    "last_months = df.groupby('zipcode')['year_month'].max().reset_index()\n",
    "last_rows = df.merge(last_months, on=['zipcode', 'year_month'], how='inner')\n",
    "\n",
    "df_marked = df.merge(last_months, on=['zipcode', 'year_month'], how='left', indicator=True)\n",
    "\n",
    "# Step 3: Keep only rows NOT in the last month\n",
    "df_filtered = df_marked[df_marked['_merge'] == 'left_only'].drop(columns=['_merge'])\n",
    "\n",
    "train = df_filtered.copy()\n",
    "\n",
    "grouped_train = train.groupby(['city', 'zipcode'])\n",
    "grouped_test = last_rows.groupby(['city', 'zipcode'])\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1154a7b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1655/471952232.py:10: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(lambda x: x.nlargest(3, 'year_month'))\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('/home/jensk/code/JensKlug/zillows_real_estate/raw_data/HouseTS.csv')\n",
    "df = df.sort_values(['city', 'zipcode', 'date']).reset_index(drop=True)\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "df['year_month'] = df['date'].dt.to_period('M')\n",
    "df = df.sort_values(['city', 'zipcode', 'year_month']).reset_index(drop=True)\n",
    "\n",
    "# Get the last 3 year_months for each zipcode\n",
    "last_3_months = (\n",
    "    df.groupby('zipcode')\n",
    "    .apply(lambda x: x.nlargest(3, 'year_month'))\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "# Create a multi-index of zipcode + year_month for the last 3 months\n",
    "last_3_keys = last_3_months[['zipcode', 'year_month']].drop_duplicates()\n",
    "\n",
    "# Merge to flag rows in last 3 months\n",
    "df_merged = df.merge(last_3_keys, on=['zipcode', 'year_month'], how='left', indicator=True)\n",
    "\n",
    "# Filter out last 3 months\n",
    "train = df_merged[df_merged['_merge'] == 'left_only'].drop(columns=['_merge'])\n",
    "\n",
    "# Optional: Create groups if needed\n",
    "grouped_train = train.groupby(['city', 'zipcode'])\n",
    "grouped_test = last_3_months.groupby(['city', 'zipcode'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6769fa99",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1655/51279733.py:13: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(lambda x: x.nlargest(3, 'year_month'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows in df: 884092\n",
      "Rows in train (should be full minus last 3 months per zipcode): 865414\n",
      "Rows in test (last 3 months per zipcode): 18678\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "'''\n",
    "df = pd.read_csv('/home/jensk/code/JensKlug/zillows_real_estate/raw_data/HouseTS.csv')\n",
    "\n",
    "df = df.sort_values(['city', 'zipcode', 'date']).reset_index(drop=True)\n",
    "\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "df['year_month'] = df['date'].dt.to_period('M')\n",
    "\n",
    "df = df.sort_values(['city', 'zipcode', 'year_month']).reset_index(drop=True)\n",
    "\n",
    "last_3_rows = (\n",
    "    df.groupby('zipcode', group_keys=False)\n",
    "    .apply(lambda x: x.nlargest(3, 'year_month'))\n",
    ")\n",
    "\n",
    "# Step 2: Create a set of (zipcode, year_month) to exclude\n",
    "exclude_set = set(zip(last_3_rows['zipcode'], last_3_rows['year_month']))\n",
    "\n",
    "# Step 3: Filter out these from the full dataset\n",
    "train = df[~df[['zipcode', 'year_month']].apply(tuple, axis=1).isin(exclude_set)]\n",
    "\n",
    "# Sanity check\n",
    "print(f\"Total rows in df: {len(df)}\")\n",
    "print(f\"Rows in train (should be full minus last 3 months per zipcode): {len(train)}\")\n",
    "print(f\"Rows in test (last 3 months per zipcode): {len(last_3_rows)}\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "6d5037f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         1431\n",
       "1         1431\n",
       "2         1431\n",
       "3         1432\n",
       "4         1432\n",
       "         ...  \n",
       "18673    98685\n",
       "18674    98685\n",
       "18675    98686\n",
       "18676    98686\n",
       "18677    98686\n",
       "Name: zipcode, Length: 18678, dtype: int64"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train['zipcode']\n",
    "last_3_months['zipcode']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "71d353f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41038    2012-03-31\n",
       "41039    2012-04-30\n",
       "41040    2012-05-31\n",
       "41180    2012-03-31\n",
       "41181    2012-04-30\n",
       "            ...    \n",
       "614009   2012-04-30\n",
       "614010   2012-05-31\n",
       "614150   2012-03-31\n",
       "614151   2012-04-30\n",
       "614152   2012-05-31\n",
       "Name: date, Length: 18678, dtype: datetime64[ns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grouped_train.head(3).sort_values(['zipcode', 'date'], ascending=(True, True))['date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2691a0b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2         1431\n",
       "1         1431\n",
       "0         1431\n",
       "5         1432\n",
       "4         1432\n",
       "         ...  \n",
       "18673    98685\n",
       "18672    98685\n",
       "18677    98686\n",
       "18676    98686\n",
       "18675    98686\n",
       "Name: zipcode, Length: 18678, dtype: int64"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#grouped_train.head(3)\n",
    "grouped_test.head(3).sort_values(['zipcode', 'date'], ascending=(True, True))['zipcode']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "992de445",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "df = pd.read_csv('/home/jensk/code/JensKlug/zillows_real_estate/raw_data/HouseTS.csv')\n",
    "\n",
    "df = df.sort_values(['zipcode', 'date']).reset_index(drop=True)\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "df['year_month'] = df['date'].dt.to_period('M')\n",
    "\n",
    "last_3_months = (\n",
    "    df.sort_values(['zipcode', 'date'], ascending=[True, False])\n",
    "      .groupby('zipcode')\n",
    "      .head(3)\n",
    ")\n",
    "\n",
    "# Optional: sort for readability\n",
    "last_3_months = last_3_months.sort_values(['zipcode', 'date']).reset_index(drop=True)\n",
    "last_rows = df.merge(last_3_months, on=['zipcode', 'year_month'], how='inner')\n",
    "\n",
    "df_marked = df.merge(last_3_months, on=['zipcode', 'year_month'], how='left', indicator=True)\n",
    "\n",
    "# Step 3: Keep only rows NOT in the last month\n",
    "df_filtered = df_marked[df_marked['_merge'] == 'left_only'].drop(columns=['_merge'])\n",
    "\n",
    "train = df_filtered.copy()\n",
    "\n",
    "grouped_train = train.groupby(['zipcode'])\n",
    "grouped_test = last_rows.groupby(['zipcode'])\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "453f2fba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>median_sale_price</th>\n",
       "      <th>median_list_price</th>\n",
       "      <th>median_ppsf</th>\n",
       "      <th>median_list_ppsf</th>\n",
       "      <th>homes_sold</th>\n",
       "      <th>pending_sales</th>\n",
       "      <th>new_listings</th>\n",
       "      <th>inventory</th>\n",
       "      <th>median_dom</th>\n",
       "      <th>...</th>\n",
       "      <th>Median Rent</th>\n",
       "      <th>Median Home Value</th>\n",
       "      <th>Total Labor Force</th>\n",
       "      <th>Unemployed Population</th>\n",
       "      <th>Total School Age Population</th>\n",
       "      <th>Total School Enrollment</th>\n",
       "      <th>Median Commute Time</th>\n",
       "      <th>price</th>\n",
       "      <th>city_full</th>\n",
       "      <th>year_month</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-12-31</td>\n",
       "      <td>461500.0</td>\n",
       "      <td>404900.0</td>\n",
       "      <td>200.601023</td>\n",
       "      <td>210.568245</td>\n",
       "      <td>8.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>57.5</td>\n",
       "      <td>...</td>\n",
       "      <td>987.0</td>\n",
       "      <td>328700.0</td>\n",
       "      <td>1921.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>3170.0</td>\n",
       "      <td>3170.0</td>\n",
       "      <td>1541.0</td>\n",
       "      <td>431806.192745</td>\n",
       "      <td>Boston-Cambridge-Newton</td>\n",
       "      <td>2023-12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-11-30</td>\n",
       "      <td>469000.0</td>\n",
       "      <td>398888.0</td>\n",
       "      <td>200.601023</td>\n",
       "      <td>210.409745</td>\n",
       "      <td>10.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>...</td>\n",
       "      <td>987.0</td>\n",
       "      <td>328700.0</td>\n",
       "      <td>1921.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>3170.0</td>\n",
       "      <td>3170.0</td>\n",
       "      <td>1541.0</td>\n",
       "      <td>431098.634112</td>\n",
       "      <td>Boston-Cambridge-Newton</td>\n",
       "      <td>2023-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-10-31</td>\n",
       "      <td>462500.0</td>\n",
       "      <td>450000.0</td>\n",
       "      <td>219.758478</td>\n",
       "      <td>191.236878</td>\n",
       "      <td>16.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>...</td>\n",
       "      <td>987.0</td>\n",
       "      <td>328700.0</td>\n",
       "      <td>1921.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>3170.0</td>\n",
       "      <td>3170.0</td>\n",
       "      <td>1541.0</td>\n",
       "      <td>429997.675464</td>\n",
       "      <td>Boston-Cambridge-Newton</td>\n",
       "      <td>2023-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-12-31</td>\n",
       "      <td>492500.0</td>\n",
       "      <td>489900.0</td>\n",
       "      <td>343.672800</td>\n",
       "      <td>287.861374</td>\n",
       "      <td>14.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>17.5</td>\n",
       "      <td>...</td>\n",
       "      <td>1333.0</td>\n",
       "      <td>397500.0</td>\n",
       "      <td>5099.0</td>\n",
       "      <td>296.0</td>\n",
       "      <td>7887.0</td>\n",
       "      <td>7887.0</td>\n",
       "      <td>3905.0</td>\n",
       "      <td>508515.970387</td>\n",
       "      <td>Boston-Cambridge-Newton</td>\n",
       "      <td>2023-12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023-11-30</td>\n",
       "      <td>476000.0</td>\n",
       "      <td>499900.0</td>\n",
       "      <td>343.023256</td>\n",
       "      <td>291.360987</td>\n",
       "      <td>17.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1333.0</td>\n",
       "      <td>397500.0</td>\n",
       "      <td>5099.0</td>\n",
       "      <td>296.0</td>\n",
       "      <td>7887.0</td>\n",
       "      <td>7887.0</td>\n",
       "      <td>3905.0</td>\n",
       "      <td>505611.783019</td>\n",
       "      <td>Boston-Cambridge-Newton</td>\n",
       "      <td>2023-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18673</th>\n",
       "      <td>2023-11-30</td>\n",
       "      <td>640000.0</td>\n",
       "      <td>599000.0</td>\n",
       "      <td>291.176471</td>\n",
       "      <td>287.162162</td>\n",
       "      <td>71.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1625.0</td>\n",
       "      <td>513000.0</td>\n",
       "      <td>15814.0</td>\n",
       "      <td>497.0</td>\n",
       "      <td>29456.0</td>\n",
       "      <td>29456.0</td>\n",
       "      <td>11629.0</td>\n",
       "      <td>577994.879622</td>\n",
       "      <td>Portland-Vancouver-Hillsboro</td>\n",
       "      <td>2023-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18674</th>\n",
       "      <td>2023-10-31</td>\n",
       "      <td>640000.0</td>\n",
       "      <td>599475.0</td>\n",
       "      <td>297.304440</td>\n",
       "      <td>294.014882</td>\n",
       "      <td>81.0</td>\n",
       "      <td>83.0</td>\n",
       "      <td>106.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1625.0</td>\n",
       "      <td>513000.0</td>\n",
       "      <td>15814.0</td>\n",
       "      <td>497.0</td>\n",
       "      <td>29456.0</td>\n",
       "      <td>29456.0</td>\n",
       "      <td>11629.0</td>\n",
       "      <td>577247.107610</td>\n",
       "      <td>Portland-Vancouver-Hillsboro</td>\n",
       "      <td>2023-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18675</th>\n",
       "      <td>2023-12-31</td>\n",
       "      <td>520200.0</td>\n",
       "      <td>519900.0</td>\n",
       "      <td>299.494207</td>\n",
       "      <td>293.822411</td>\n",
       "      <td>101.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1677.0</td>\n",
       "      <td>497100.0</td>\n",
       "      <td>10143.0</td>\n",
       "      <td>395.0</td>\n",
       "      <td>19904.0</td>\n",
       "      <td>19904.0</td>\n",
       "      <td>7787.0</td>\n",
       "      <td>569875.467016</td>\n",
       "      <td>Portland-Vancouver-Hillsboro</td>\n",
       "      <td>2023-12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18676</th>\n",
       "      <td>2023-11-30</td>\n",
       "      <td>525000.0</td>\n",
       "      <td>532580.0</td>\n",
       "      <td>309.415584</td>\n",
       "      <td>296.314311</td>\n",
       "      <td>118.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>30.5</td>\n",
       "      <td>...</td>\n",
       "      <td>1677.0</td>\n",
       "      <td>497100.0</td>\n",
       "      <td>10143.0</td>\n",
       "      <td>395.0</td>\n",
       "      <td>19904.0</td>\n",
       "      <td>19904.0</td>\n",
       "      <td>7787.0</td>\n",
       "      <td>569327.047815</td>\n",
       "      <td>Portland-Vancouver-Hillsboro</td>\n",
       "      <td>2023-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18677</th>\n",
       "      <td>2023-10-31</td>\n",
       "      <td>525000.0</td>\n",
       "      <td>504450.0</td>\n",
       "      <td>306.527550</td>\n",
       "      <td>301.147755</td>\n",
       "      <td>135.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1677.0</td>\n",
       "      <td>497100.0</td>\n",
       "      <td>10143.0</td>\n",
       "      <td>395.0</td>\n",
       "      <td>19904.0</td>\n",
       "      <td>19904.0</td>\n",
       "      <td>7787.0</td>\n",
       "      <td>568266.780424</td>\n",
       "      <td>Portland-Vancouver-Hillsboro</td>\n",
       "      <td>2023-10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>18678 rows × 40 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            date  median_sale_price  median_list_price  median_ppsf  \\\n",
       "0     2023-12-31           461500.0           404900.0   200.601023   \n",
       "1     2023-11-30           469000.0           398888.0   200.601023   \n",
       "2     2023-10-31           462500.0           450000.0   219.758478   \n",
       "3     2023-12-31           492500.0           489900.0   343.672800   \n",
       "4     2023-11-30           476000.0           499900.0   343.023256   \n",
       "...          ...                ...                ...          ...   \n",
       "18673 2023-11-30           640000.0           599000.0   291.176471   \n",
       "18674 2023-10-31           640000.0           599475.0   297.304440   \n",
       "18675 2023-12-31           520200.0           519900.0   299.494207   \n",
       "18676 2023-11-30           525000.0           532580.0   309.415584   \n",
       "18677 2023-10-31           525000.0           504450.0   306.527550   \n",
       "\n",
       "       median_list_ppsf  homes_sold  pending_sales  new_listings  inventory  \\\n",
       "0            210.568245         8.0           11.0          14.0        8.0   \n",
       "1            210.409745        10.0            9.0          11.0        9.0   \n",
       "2            191.236878        16.0           10.0           9.0        7.0   \n",
       "3            287.861374        14.0           16.0          18.0        6.0   \n",
       "4            291.360987        17.0           16.0          19.0        8.0   \n",
       "...                 ...         ...            ...           ...        ...   \n",
       "18673        287.162162        71.0           75.0          99.0       69.0   \n",
       "18674        294.014882        81.0           83.0         106.0       72.0   \n",
       "18675        293.822411       101.0           73.0          72.0       63.0   \n",
       "18676        296.314311       118.0           78.0          84.0       70.0   \n",
       "18677        301.147755       135.0           95.0          94.0       61.0   \n",
       "\n",
       "       median_dom  ...  Median Rent  Median Home Value  Total Labor Force  \\\n",
       "0            57.5  ...        987.0           328700.0             1921.0   \n",
       "1            32.0  ...        987.0           328700.0             1921.0   \n",
       "2            22.0  ...        987.0           328700.0             1921.0   \n",
       "3            17.5  ...       1333.0           397500.0             5099.0   \n",
       "4            17.0  ...       1333.0           397500.0             5099.0   \n",
       "...           ...  ...          ...                ...                ...   \n",
       "18673        27.0  ...       1625.0           513000.0            15814.0   \n",
       "18674        21.0  ...       1625.0           513000.0            15814.0   \n",
       "18675        31.0  ...       1677.0           497100.0            10143.0   \n",
       "18676        30.5  ...       1677.0           497100.0            10143.0   \n",
       "18677        19.0  ...       1677.0           497100.0            10143.0   \n",
       "\n",
       "      Unemployed Population  Total School Age Population  \\\n",
       "0                      97.0                       3170.0   \n",
       "1                      97.0                       3170.0   \n",
       "2                      97.0                       3170.0   \n",
       "3                     296.0                       7887.0   \n",
       "4                     296.0                       7887.0   \n",
       "...                     ...                          ...   \n",
       "18673                 497.0                      29456.0   \n",
       "18674                 497.0                      29456.0   \n",
       "18675                 395.0                      19904.0   \n",
       "18676                 395.0                      19904.0   \n",
       "18677                 395.0                      19904.0   \n",
       "\n",
       "       Total School Enrollment  Median Commute Time          price  \\\n",
       "0                       3170.0               1541.0  431806.192745   \n",
       "1                       3170.0               1541.0  431098.634112   \n",
       "2                       3170.0               1541.0  429997.675464   \n",
       "3                       7887.0               3905.0  508515.970387   \n",
       "4                       7887.0               3905.0  505611.783019   \n",
       "...                        ...                  ...            ...   \n",
       "18673                  29456.0              11629.0  577994.879622   \n",
       "18674                  29456.0              11629.0  577247.107610   \n",
       "18675                  19904.0               7787.0  569875.467016   \n",
       "18676                  19904.0               7787.0  569327.047815   \n",
       "18677                  19904.0               7787.0  568266.780424   \n",
       "\n",
       "                          city_full  year_month  \n",
       "0           Boston-Cambridge-Newton     2023-12  \n",
       "1           Boston-Cambridge-Newton     2023-11  \n",
       "2           Boston-Cambridge-Newton     2023-10  \n",
       "3           Boston-Cambridge-Newton     2023-12  \n",
       "4           Boston-Cambridge-Newton     2023-11  \n",
       "...                             ...         ...  \n",
       "18673  Portland-Vancouver-Hillsboro     2023-11  \n",
       "18674  Portland-Vancouver-Hillsboro     2023-10  \n",
       "18675  Portland-Vancouver-Hillsboro     2023-12  \n",
       "18676  Portland-Vancouver-Hillsboro     2023-11  \n",
       "18677  Portland-Vancouver-Hillsboro     2023-10  \n",
       "\n",
       "[18678 rows x 40 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df.head(3)\n",
    "last_3_months\n",
    "#last_months\n",
    "#last_rows\n",
    "#df_filtered.iloc[120:151] # Here we see that December is missing in 2023. :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c097ef91",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1904/871680835.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ts_train[f'{feature}_{lag}m'] = grouped_train[f'{feature}'].shift(lag)\n",
      "/tmp/ipykernel_1904/871680835.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ts_train[f'{feature}_{lag}m'] = grouped_train[f'{feature}'].shift(lag)\n",
      "/tmp/ipykernel_1904/871680835.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ts_train[f'{feature}_{lag}m'] = grouped_train[f'{feature}'].shift(lag)\n",
      "/tmp/ipykernel_1904/871680835.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ts_train[f'{feature}_{lag}m'] = grouped_train[f'{feature}'].shift(lag)\n",
      "/tmp/ipykernel_1904/871680835.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ts_train[f'{feature}_{lag}m'] = grouped_train[f'{feature}'].shift(lag)\n",
      "/tmp/ipykernel_1904/871680835.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ts_train[f'{feature}_{lag}m'] = grouped_train[f'{feature}'].shift(lag)\n",
      "/tmp/ipykernel_1904/871680835.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ts_train[f'{feature}_{lag}m'] = grouped_train[f'{feature}'].shift(lag)\n",
      "/tmp/ipykernel_1904/871680835.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ts_train[f'{feature}_{lag}m'] = grouped_train[f'{feature}'].shift(lag)\n",
      "/tmp/ipykernel_1904/871680835.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ts_train[f'{feature}_{lag}m'] = grouped_train[f'{feature}'].shift(lag)\n",
      "/tmp/ipykernel_1904/871680835.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ts_train[f'{feature}_{lag}m'] = grouped_train[f'{feature}'].shift(lag)\n",
      "/tmp/ipykernel_1904/871680835.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ts_test[f'{feature}_{lag}m'] = grouped_test[f'{feature}'].shift(lag)\n",
      "/tmp/ipykernel_1904/871680835.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ts_test[f'{feature}_{lag}m'] = grouped_test[f'{feature}'].shift(lag)\n",
      "/tmp/ipykernel_1904/871680835.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ts_test[f'{feature}_{lag}m'] = grouped_test[f'{feature}'].shift(lag)\n",
      "/tmp/ipykernel_1904/871680835.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ts_test[f'{feature}_{lag}m'] = grouped_test[f'{feature}'].shift(lag)\n",
      "/tmp/ipykernel_1904/871680835.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ts_test[f'{feature}_{lag}m'] = grouped_test[f'{feature}'].shift(lag)\n",
      "/tmp/ipykernel_1904/871680835.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ts_test[f'{feature}_{lag}m'] = grouped_test[f'{feature}'].shift(lag)\n",
      "/tmp/ipykernel_1904/871680835.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ts_test[f'{feature}_{lag}m'] = grouped_test[f'{feature}'].shift(lag)\n",
      "/tmp/ipykernel_1904/871680835.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ts_test[f'{feature}_{lag}m'] = grouped_test[f'{feature}'].shift(lag)\n",
      "/tmp/ipykernel_1904/871680835.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ts_test[f'{feature}_{lag}m'] = grouped_test[f'{feature}'].shift(lag)\n",
      "/tmp/ipykernel_1904/871680835.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ts_test[f'{feature}_{lag}m'] = grouped_test[f'{feature}'].shift(lag)\n"
     ]
    }
   ],
   "source": [
    "ts_train = pd.DataFrame()\n",
    "\n",
    "ls = [1,3,6,12]\n",
    "\n",
    "i = 1\n",
    "\n",
    "#for i in ls:\n",
    "ts_train[f'target_profitable_{i}m'] = (grouped_train['price'].shift(-i) > 1.02 * train['price']).astype(int) # We fail to exclude the last three obervastions since with the shaft they are NaN!\n",
    "ts_train['zipcode'] = grouped_train['zipcode'].shift(0)# .astype('object') # but remains still an integer...\n",
    "numeric_column_names = train.select_dtypes(include='number').columns # drop the lags of zipcode\n",
    "\n",
    "for feature in numeric_column_names:\n",
    "    for lag in range(1, 4):  # 1 to 3 months\n",
    "        ts_train[f'{feature}_{lag}m'] = grouped_train[f'{feature}'].shift(lag)\n",
    "\n",
    "\n",
    "ts_test = pd.DataFrame()\n",
    "\n",
    "#for i in ls:\n",
    "ts_test[f'target_profitable_{i}m'] = (grouped_test['price'].shift(-i) > 1.02 * last_rows['price']).astype(int) # We fail to exclude the last three obervastions since with the shaft they are NaN!\n",
    "ts_test['zipcode'] = grouped_test['zipcode'].shift(0)# .astype('object') # but remains still an integer...\n",
    "numeric_column_names = last_rows.select_dtypes(include='number').columns # drop the lags of zipcode\n",
    "\n",
    "for feature in numeric_column_names:\n",
    "    for lag in range(1, 4):  # 1 to 3 months\n",
    "        ts_test[f'{feature}_{lag}m'] = grouped_test[f'{feature}'].shift(lag)\n",
    "\n",
    "#ts_test = ts_test.dropna().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5a8d993b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((6226, 110),\n",
       "       target_profitable_1m  zipcode  median_sale_price_1m  \\\n",
       " 0                        0    30002                   NaN   \n",
       " 1                        0    30004                   NaN   \n",
       " 2                        0    30005                   NaN   \n",
       " 3                        0    30008                   NaN   \n",
       " 4                        0    30009                   NaN   \n",
       " ...                    ...      ...                   ...   \n",
       " 6221                     0    34689                   NaN   \n",
       " 6222                     0    34690                   NaN   \n",
       " 6223                     0    34691                   NaN   \n",
       " 6224                     0    34695                   NaN   \n",
       " 6225                     0    34698                   NaN   \n",
       " \n",
       "       median_sale_price_2m  median_sale_price_3m  median_list_price_1m  \\\n",
       " 0                      NaN                   NaN                   NaN   \n",
       " 1                      NaN                   NaN                   NaN   \n",
       " 2                      NaN                   NaN                   NaN   \n",
       " 3                      NaN                   NaN                   NaN   \n",
       " 4                      NaN                   NaN                   NaN   \n",
       " ...                    ...                   ...                   ...   \n",
       " 6221                   NaN                   NaN                   NaN   \n",
       " 6222                   NaN                   NaN                   NaN   \n",
       " 6223                   NaN                   NaN                   NaN   \n",
       " 6224                   NaN                   NaN                   NaN   \n",
       " 6225                   NaN                   NaN                   NaN   \n",
       " \n",
       "       median_list_price_2m  median_list_price_3m  median_ppsf_1m  \\\n",
       " 0                      NaN                   NaN             NaN   \n",
       " 1                      NaN                   NaN             NaN   \n",
       " 2                      NaN                   NaN             NaN   \n",
       " 3                      NaN                   NaN             NaN   \n",
       " 4                      NaN                   NaN             NaN   \n",
       " ...                    ...                   ...             ...   \n",
       " 6221                   NaN                   NaN             NaN   \n",
       " 6222                   NaN                   NaN             NaN   \n",
       " 6223                   NaN                   NaN             NaN   \n",
       " 6224                   NaN                   NaN             NaN   \n",
       " 6225                   NaN                   NaN             NaN   \n",
       " \n",
       "       median_ppsf_2m  ...  Total School Age Population_3m  \\\n",
       " 0                NaN  ...                             NaN   \n",
       " 1                NaN  ...                             NaN   \n",
       " 2                NaN  ...                             NaN   \n",
       " 3                NaN  ...                             NaN   \n",
       " 4                NaN  ...                             NaN   \n",
       " ...              ...  ...                             ...   \n",
       " 6221             NaN  ...                             NaN   \n",
       " 6222             NaN  ...                             NaN   \n",
       " 6223             NaN  ...                             NaN   \n",
       " 6224             NaN  ...                             NaN   \n",
       " 6225             NaN  ...                             NaN   \n",
       " \n",
       "       Total School Enrollment_1m  Total School Enrollment_2m  \\\n",
       " 0                            NaN                         NaN   \n",
       " 1                            NaN                         NaN   \n",
       " 2                            NaN                         NaN   \n",
       " 3                            NaN                         NaN   \n",
       " 4                            NaN                         NaN   \n",
       " ...                          ...                         ...   \n",
       " 6221                         NaN                         NaN   \n",
       " 6222                         NaN                         NaN   \n",
       " 6223                         NaN                         NaN   \n",
       " 6224                         NaN                         NaN   \n",
       " 6225                         NaN                         NaN   \n",
       " \n",
       "       Total School Enrollment_3m  Median Commute Time_1m  \\\n",
       " 0                            NaN                     NaN   \n",
       " 1                            NaN                     NaN   \n",
       " 2                            NaN                     NaN   \n",
       " 3                            NaN                     NaN   \n",
       " 4                            NaN                     NaN   \n",
       " ...                          ...                     ...   \n",
       " 6221                         NaN                     NaN   \n",
       " 6222                         NaN                     NaN   \n",
       " 6223                         NaN                     NaN   \n",
       " 6224                         NaN                     NaN   \n",
       " 6225                         NaN                     NaN   \n",
       " \n",
       "       Median Commute Time_2m  Median Commute Time_3m  price_1m  price_2m  \\\n",
       " 0                        NaN                     NaN       NaN       NaN   \n",
       " 1                        NaN                     NaN       NaN       NaN   \n",
       " 2                        NaN                     NaN       NaN       NaN   \n",
       " 3                        NaN                     NaN       NaN       NaN   \n",
       " 4                        NaN                     NaN       NaN       NaN   \n",
       " ...                      ...                     ...       ...       ...   \n",
       " 6221                     NaN                     NaN       NaN       NaN   \n",
       " 6222                     NaN                     NaN       NaN       NaN   \n",
       " 6223                     NaN                     NaN       NaN       NaN   \n",
       " 6224                     NaN                     NaN       NaN       NaN   \n",
       " 6225                     NaN                     NaN       NaN       NaN   \n",
       " \n",
       "       price_3m  \n",
       " 0          NaN  \n",
       " 1          NaN  \n",
       " 2          NaN  \n",
       " 3          NaN  \n",
       " 4          NaN  \n",
       " ...        ...  \n",
       " 6221       NaN  \n",
       " 6222       NaN  \n",
       " 6223       NaN  \n",
       " 6224       NaN  \n",
       " 6225       NaN  \n",
       " \n",
       " [6226 rows x 110 columns])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ts_test.shape, ts_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d7d712fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_test = ts_test.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "205c377e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "target_profitable_1m\n",
       "0    838715\n",
       "1     39151\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ts_train['target_profitable_1m'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0bc7e075",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_train = ts_train.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4c61e53b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_train = ts_train.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4e869394",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = ts_train.drop(columns=[f'target_profitable_{i}m'])\n",
    "y_train = ts_train[f'target_profitable_{i}m']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4bc9878e",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "417d6598",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(859188, 109)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_scaled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "715eeee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = ts_test.drop(columns=[f'target_profitable_{i}m'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "896435f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((6226, 109), (0, 110))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred.shape, ts_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a6191b0f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input X contains NaN.\nSGDClassifier does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m model \u001b[38;5;241m=\u001b[39m SGDClassifier(loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlog_loss\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m model\u001b[38;5;241m.\u001b[39mfit(X_train_scaled, y_train)\n\u001b[0;32m----> 6\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_pred_scaled\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m np\u001b[38;5;241m.\u001b[39msavetxt(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mmm.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, y_pred, delimiter\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m, header\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtarget_-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mm\u001b[39m\u001b[38;5;124m\"\u001b[39m, comments\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m y_pred\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/zillows_real_estate/lib/python3.10/site-packages/sklearn/linear_model/_base.py:375\u001b[0m, in \u001b[0;36mLinearClassifierMixin.predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    361\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    362\u001b[0m \u001b[38;5;124;03mPredict class labels for samples in X.\u001b[39;00m\n\u001b[1;32m    363\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    372\u001b[0m \u001b[38;5;124;03m    Vector containing the class labels for each sample.\u001b[39;00m\n\u001b[1;32m    373\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    374\u001b[0m xp, _ \u001b[38;5;241m=\u001b[39m get_namespace(X)\n\u001b[0;32m--> 375\u001b[0m scores \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecision_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    376\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(scores\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    377\u001b[0m     indices \u001b[38;5;241m=\u001b[39m xp\u001b[38;5;241m.\u001b[39mastype(scores \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m, indexing_dtype(xp))\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/zillows_real_estate/lib/python3.10/site-packages/sklearn/linear_model/_base.py:352\u001b[0m, in \u001b[0;36mLinearClassifierMixin.decision_function\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    349\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m    350\u001b[0m xp, _ \u001b[38;5;241m=\u001b[39m get_namespace(X)\n\u001b[0;32m--> 352\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mvalidate_data\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    353\u001b[0m scores \u001b[38;5;241m=\u001b[39m safe_sparse_dot(X, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcoef_\u001b[38;5;241m.\u001b[39mT, dense_output\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintercept_\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m    355\u001b[0m     xp\u001b[38;5;241m.\u001b[39mreshape(scores, (\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,))\n\u001b[1;32m    356\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (scores\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m scores\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    357\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m scores\n\u001b[1;32m    358\u001b[0m )\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/zillows_real_estate/lib/python3.10/site-packages/sklearn/utils/validation.py:2954\u001b[0m, in \u001b[0;36mvalidate_data\u001b[0;34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001b[0m\n\u001b[1;32m   2952\u001b[0m         out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[1;32m   2953\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m no_val_y:\n\u001b[0;32m-> 2954\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mX\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2955\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_y:\n\u001b[1;32m   2956\u001b[0m     out \u001b[38;5;241m=\u001b[39m _check_y(y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/zillows_real_estate/lib/python3.10/site-packages/sklearn/utils/validation.py:1105\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_non_negative, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m   1099\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1100\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound array with dim \u001b[39m\u001b[38;5;132;01m{\u001b[39;00marray\u001b[38;5;241m.\u001b[39mndim\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1101\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m while dim <= 2 is required\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcontext\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1102\u001b[0m     )\n\u001b[1;32m   1104\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ensure_all_finite:\n\u001b[0;32m-> 1105\u001b[0m     \u001b[43m_assert_all_finite\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1106\u001b[0m \u001b[43m        \u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1107\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1108\u001b[0m \u001b[43m        \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1109\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_all_finite\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallow-nan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1110\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1112\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m copy:\n\u001b[1;32m   1113\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_numpy_namespace(xp):\n\u001b[1;32m   1114\u001b[0m         \u001b[38;5;66;03m# only make a copy if `array` and `array_orig` may share memory`\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/zillows_real_estate/lib/python3.10/site-packages/sklearn/utils/validation.py:120\u001b[0m, in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m first_pass_isfinite:\n\u001b[1;32m    118\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m--> 120\u001b[0m \u001b[43m_assert_all_finite_element_wise\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    121\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[43m    \u001b[49m\u001b[43mxp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mxp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    123\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_nan\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    124\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmsg_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmsg_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    125\u001b[0m \u001b[43m    \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    126\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    127\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/zillows_real_estate/lib/python3.10/site-packages/sklearn/utils/validation.py:169\u001b[0m, in \u001b[0;36m_assert_all_finite_element_wise\u001b[0;34m(X, xp, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m estimator_name \u001b[38;5;129;01mand\u001b[39;00m input_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m has_nan_error:\n\u001b[1;32m    153\u001b[0m     \u001b[38;5;66;03m# Improve the error message on how to handle missing values in\u001b[39;00m\n\u001b[1;32m    154\u001b[0m     \u001b[38;5;66;03m# scikit-learn.\u001b[39;00m\n\u001b[1;32m    155\u001b[0m     msg_err \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    156\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not accept missing values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    157\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m encoded as NaN natively. For supervised learning, you might want\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    167\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#estimators-that-handle-nan-values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    168\u001b[0m     )\n\u001b[0;32m--> 169\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg_err)\n",
      "\u001b[0;31mValueError\u001b[0m: Input X contains NaN.\nSGDClassifier does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values"
     ]
    }
   ],
   "source": [
    "\n",
    "X_pred_scaled = scaler.transform(pred)\n",
    "\n",
    "model = SGDClassifier(loss='log_loss')\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "y_pred = model.predict(X_pred_scaled)\n",
    "\n",
    "np.savetxt(f\"output_{i}mm.csv\", y_pred, delimiter=\",\", header=f\"target_-{i}m\", comments='')\n",
    "\n",
    "del y_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c434f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "zipcode_identifier = ts_test['zipcode']\n",
    "zipcode_identifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "155b76b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_test_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba8787f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d2a9a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_1 = pd.read_csv('output_1m.csv')\n",
    "output_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d1ad6cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_2 = pd.read_csv('output_2m.csv')\n",
    "output_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d17ba0d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined = pd.concat([zipcode_identifier, output_1, output_2], axis=1)\n",
    "df_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c28809",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_result = df_combined.groupby(['zipcode']).mean()\n",
    "grouped_result.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf9a6087",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''ts_train = pd.DataFrame()\n",
    "ts_test = pd.DataFrame()\n",
    "\n",
    "#ls = [1,3,6,12]\n",
    "\n",
    "\n",
    "\n",
    "#for i in ls:\n",
    "ts_train['target_profitable_3m'] = (grouped_train['price'].shift(-6) > 1.02 * grouped_train['price'].shift(0)).astype(int) # We fail to exclude the last three obervastions since with the shaft they are NaN!\n",
    "\n",
    "numeric_column_names = train.select_dtypes(include='number').columns # drop the lags of zipcode\n",
    "\n",
    "for feature in numeric_column_names:\n",
    "    for lag in range(1, 4):  # 1 to 3 months\n",
    "        ts_train[f'{feature}_{lag}m'] = grouped_train[f'{feature}'].shift(lag)\n",
    "\n",
    "ts_train = ts_train.dropna().reset_index(drop=True)\n",
    "\n",
    "X_train = ts_train.drop(columns=['target_profitable_3m'])\n",
    "y_train = ts_train['target_profitable_3m']\n",
    "\n",
    "\n",
    "ts_test['target_profitable_3m'] = (grouped_test['price'].shift(-6) > 1.02 * grouped_test['price'].shift(0)).astype(int)\n",
    "\n",
    "numeric_column_names = test.select_dtypes(include='number').columns # drop the lags of zipcode\n",
    "\n",
    "for feature in numeric_column_names:\n",
    "    for lag in range(1, 4):  # 1 to 6 months\n",
    "        ts_test[f'{feature}_{lag}m'] = grouped_test[f'{feature}'].shift(lag)\n",
    "\n",
    "ts_test = ts_test.dropna().reset_index(drop=True)\n",
    "\n",
    "X_test = ts_test.drop(columns=['target_profitable_3m'])\n",
    "y_test = ts_test['target_profitable_3m']\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "model = SGDClassifier(loss='log_loss')\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''ts_train = pd.DataFrame()\n",
    "ts_test = pd.DataFrame()\n",
    "\n",
    "\n",
    "ls = [1,3,6,12]\n",
    "\n",
    "for i in ls:\n",
    "    ts_train[f'target_profitable_{i}m'] = (grouped_train['price'].shift(-i) > 1.02 * grouped_train['price'].shift(0)).astype(int) # We fail to exclude the last three obervastions since with the shaft they are NaN!\n",
    "\n",
    "    numeric_column_names = train.select_dtypes(include='number').columns # drop the lags of zipcode\n",
    "\n",
    "    for feature in numeric_column_names:\n",
    "        for lag in range(1, 4):  # 1 to 3 months\n",
    "            ts_train[f'{feature}_{lag}m'] = grouped_train[f'{feature}'].shift(lag)\n",
    "\n",
    "    ts_train = ts_train.dropna().reset_index(drop=True)\n",
    "\n",
    "    X_train = ts_train.drop(columns=[f'target_profitable_{i}m'])\n",
    "    y_train = ts_train[f'target_profitable_{i}m']\n",
    "\n",
    "\n",
    "    ts_test[f'target_profitable_{i}m'] = (grouped_test['price'].shift(-i) > 1.02 * grouped_test['price'].shift(0)).astype(int)\n",
    "\n",
    "    numeric_column_names = test.select_dtypes(include='number').columns # drop the lags of zipcode\n",
    "\n",
    "    for feature in numeric_column_names:\n",
    "        for lag in range(1, 4):  # 1 to 6 months\n",
    "            ts_test[f'{feature}_{lag}m'] = grouped_test[f'{feature}'].shift(lag)\n",
    "\n",
    "    ts_test = ts_test.dropna().reset_index(drop=True)\n",
    "\n",
    "    X_test = ts_test.drop(columns=[f'target_profitable_{i}m'])\n",
    "    y_test = ts_test[f'target_profitable_{i}m']\n",
    "\n",
    "    scaler = MinMaxScaler()\n",
    "    scaler.fit(X_train)\n",
    "    X_train_scaled = scaler.transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    model = SGDClassifier(loss='log_loss')\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "    np.savetxt(f\"output_{i}m.csv\", y_pred, delimiter=\",\", header=f\"target_-{i}m\", comments='')\n",
    "\n",
    "    del y_pred\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zillows_real_estate",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
